## Why ResNet

设有深度为 $d$ 的神经网络，目标函数为 $f$ 假设第 $t$ 层神经网络可以表示的空间为 $S_t$，若 $S_t$ 表示的空间包含于 $S_{t-1}$ 表示的空间，则$S_t$ 一定会更逼近与 $f$ ，若 $S_t \cup S_{t-1}$ not equal to $S_t$ 则说明两个空间有不 **互嵌**，说明 $S_{t-1}$ 可能表示的空间更接近与$f$ ，导致更深层的神经网络精度却降低

在原始的神经网络中，由于层与层之间只是单纯的**叠加**，所以对于第 $i$ 层需要学习的目标函数即为单纯的$H(x_{i-1})$ 

但是在残差神经网络中，第 $i$ 层学习的目标函数成为了 $H(x_{i-1}) - x_{i-1}$ `残差`，所以称之为残差神经网络

灵感来源：假设深度为 $d$ 的神经网络得到了最优解，那么对于深度为 $d + n$ 的神经网络，令前 $d$ 层的参数与最优解相同，后 $n$ 层为恒等变化（identity mapping），则此深度为 $d + n$ 的神经网络亦可以得到最优解
## Convolution

使用 $1 \times 1$ 的卷积，进行升维或降维的操作，使得传入残差的通道数和经过卷积处理之后的通道数一致。