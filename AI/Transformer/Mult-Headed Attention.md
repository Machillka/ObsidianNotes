（自己对于多头注意力机制的理解）

### 协助理解

因为涉及到 Q ( Query ), K ( Key ), V ( Value ) 矩阵，很容易联想到数据库中的查询操作。

哈希表：唯一的 Key -> Value 的对应关系


## 自注意力操作过程

1. 先对于输入进行 embedding 操作，再嵌入单词位置信息，得到处理后的向量X
2. 对于 $QK^T = Res_1$ 相当于一次查询，对于当前输入信息的一次查询
3. 将查询的结果 $Res_1$ 进行 $Attention = Softmax(\frac{Res_1}{\sqrt{d_k}}) \times V$ 得到梯度比较平滑的注意力
4. 将 res 投喂给一个线性层, 得到最终的 Attention 结果

## 多头注意力机制操作过程

将 $Q, V, K$ 拆分成多个向量, 分别进行自注意力过程, 然后将结果拼接, 得到多头注意力的结果 
