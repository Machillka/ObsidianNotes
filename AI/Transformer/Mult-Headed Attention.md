（自己对于多头注意力机制的理解）

### 协助理解

因为涉及到 Q ( Query ), K ( Key ), V ( Value ) 矩阵，很容易联想到数据库中的查询操作。

哈希表：唯一的 Key -> Value 的对应关系


## 自注意力操作过程

1. 先对于输入进行 embedding 操作，再嵌入单词位置信息，得到处理后的向量X
2. 对于 $QK^T = Res_1$ 相当于一次查询，对于当前输入信息的一次查询
3. 将查询的结果 $Res_1$ 进行 $Attention = Softmax(\frac{Res_1}{\sqrt{d_k}}) \times V$ 得到梯度比较平滑的注意力
4. 将 res 投喂给一个线性层, 得到最终的 Attention 结果

其中Attention的结果中, t 行表示第 t 个词与其他词语的注意力结果（因为每一个输入向量都与其他向量做了内积，内积也可以说明余弦相似度）。所以只需要简单的矩阵乘法就可以得到所有词语的Attention结果，效率、并行度高


## 多头注意力机制操作过程

将 $Q, V, K$ 拆分成多个向量, 分别进行自注意力过程, 然后将结果拼接, 得到多头注意力的结果


## Mask


## Summary

所以 Attention 类似于一种加权求和的结果, 其中权重就是 Q 和 K 的相似度，相似度越高，表示注意力在此时刻（当前Query）越集中在相似度高的词语上面，达到了对于序列全局分析的效果